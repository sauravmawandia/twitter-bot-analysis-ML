{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insult Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we would like to filter out insulting comments on a web forum. \n",
    "\n",
    "To train our models, we have a list of historic comments with a judgement wether they're insulting or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>You fuck your dad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>i really don't understand your point.  It seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A majority of Canadians can and has been wrong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>listen if you dont wanna get married to a man ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>Các bạn xuống đường biểu tình 2011 có ôn hoà k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620171226Z</td>\n",
       "      <td>@SDL OK, but I would hope they'd sign him to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>20120503012628Z</td>\n",
       "      <td>Yeah and where are you now?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shut the fuck up. you and the rest of your fag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502173553Z</td>\n",
       "      <td>Either you are fake or extremely stupid...mayb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>20120620160512Z</td>\n",
       "      <td>That you are an idiot who understands neither ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment\n",
       "0       1  20120618192155Z                                 You fuck your dad.\n",
       "1       0  20120528192215Z  i really don't understand your point.  It seem...\n",
       "2       0              NaN  A majority of Canadians can and has been wrong...\n",
       "3       0              NaN  listen if you dont wanna get married to a man ...\n",
       "4       0  20120619094753Z  Các bạn xuống đường biểu tình 2011 có ôn hoà k...\n",
       "5       0  20120620171226Z  @SDL OK, but I would hope they'd sign him to a...\n",
       "6       0  20120503012628Z                        Yeah and where are you now?\n",
       "7       1              NaN  shut the fuck up. you and the rest of your fag...\n",
       "8       1  20120502173553Z  Either you are fake or extremely stupid...mayb...\n",
       "9       1  20120620160512Z  That you are an idiot who understands neither ..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path_to_insults = 'data/'\n",
    "data = pd.read_csv(path_to_insults + 'train-utf8.csv')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3947 comments, of which 1049 insults (26%)\n"
     ]
    }
   ],
   "source": [
    "print (\"%d comments, of which %d insults (%d%%)\" % \\\n",
    "    (len(data), data.Insult.sum(), 100 * data.Insult.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for known bad words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to do this, is to load Google's bad word list and flag comments that contain one or more words.\n",
    "\n",
    "- Load `google_badlist.txt` from `data/insults/`\n",
    "- Add a column to `data` with a flag (0 or 1) if the comment contains a bad word\n",
    "- Compute the accuracy of this method - does this look good?\n",
    "- What would a naive classifier's score be (i.e., always predicting 0 or 1)?\n",
    "- Also compute the precision, recall, F1 score and AUC score\n",
    "- What is your verdict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/google_badlist.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = path_to_insults + 'google_badlist.txt'\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_words = pd.read_table(filename,header=None,names=['badWord'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isBadWord(doc):\n",
    "    if any([word in doc.split() for word in bad_words.badWord] ):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['prediction']=data.Comment.apply(lambda x:isBadWord(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.81327590575121"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (float(len(data[data[\"Insult\"] == data[\"prediction\"]])) / len(data))* 100\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The accuracy should be more, this seems to be low "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(data[\"Comment\"])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "X_train_counts, data.Insult, test_size=0.5, random_state=0)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.01418439716312"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, Y_test) * 100\n",
    "#accuracy after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.91      0.86      1429\n",
      "          1       0.65      0.45      0.53       545\n",
      "\n",
      "avg / total       0.77      0.78      0.77      1974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#naive classifier with 50% training data\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(Y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.87      0.81      2898\n",
      "          1       0.42      0.27      0.33      1049\n",
      "\n",
      "avg / total       0.68      0.71      0.68      3947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(data[\"Insult\"], data[\"prediction\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It seems that naive bayes is better than the previous model.\n",
    "As precison, recall, f1-score is high in naive bayes for insults.\n",
    "\n",
    "Thus Naive bayes seems to be a clear winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning bad words on the fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of doing this, is to learn the insulting words on the fly using `CountVectorizer`. \n",
    "\n",
    "Please refer to the scikit learn tutorial at 'http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html' if you need some help.\n",
    "\n",
    "Here is what you need to do:\n",
    "\n",
    "- Import `CountVectorizer` from `sklearn.feature_extraction.text`\n",
    "- Train the `CountVectorizer` on the insults and create a feature set $X$ representing words in the comments\n",
    "- Train `MultinomialNB` and `BernoulliNB` from `scikitsklearn`  on the new feature set $X$\n",
    "- Using cross-validation, compute the accuracy, precision, recall, F1 and AUC of your model\n",
    "- What is your verdict?\n",
    "\n",
    "NOTE: The F1 score is another useful score to compute when one of the two classes is very rare. We didn't go over it in class but it's basically the harmonic mean between precision and recall and goes from 0 (min) to 1 (max).  You can see more here: 'https://en.wikipedia.org/wiki/F1_score' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saura\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del data['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(data[\"Comment\"])\n",
    "clf_multinomial=MultinomialNB()\n",
    "clf_multinomial.fit(X_train_counts, data.Insult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf_binomial=BernoulliNB()\n",
    "clf_binomial.fit(X_train_counts, data.Insult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is with mean= 0.740310924143 and SD= 0.00779666420524\n",
      "precision is with mean= 0.556391121316 and SD= 0.0814113532582\n",
      "f1 is with mean= 0.16810091495 and SD= 0.0383787845808\n",
      "recall is with mean= 0.0991622322281 and SD= 0.0239296795615\n",
      "roc_auc is with mean= 0.819083259814 and SD= 0.00257648952437\n"
     ]
    }
   ],
   "source": [
    "#Bernoulli\n",
    "from sklearn.model_selection import cross_val_score\n",
    "metrics= ['accuracy', 'precision', 'f1', 'recall', 'roc_auc']\n",
    "for metric in metrics:\n",
    "    sc=cross_val_score(BernoulliNB(), X_train_counts, data[\"Insult\"],scoring=metric)\n",
    "    print(metric,\"is with mean=\",sc.mean() ,\"and SD=\",sc.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is with mean= 0.787687080333 and SD= 0.000891868788176\n",
      "precision is with mean= 0.605393351267 and SD= 0.0052512399194\n",
      "f1 is with mean= 0.591467682267 and SD= 0.00735937178716\n",
      "recall is with mean= 0.578621912949 and SD= 0.018260974227\n",
      "roc_auc is with mean= 0.788669213626 and SD= 0.00671398358448\n"
     ]
    }
   ],
   "source": [
    "#Multinomial\n",
    "for metric in metrics:\n",
    "    sc=cross_val_score(MultinomialNB(), X_train_counts, data[\"Insult\"],scoring=metric)\n",
    "    print(metric,\"is with mean=\",sc.mean() ,\"and SD=\",sc.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verdict \n",
    "Clearly Multinomial is better than bernoulli as all the metrics has a higher value \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
